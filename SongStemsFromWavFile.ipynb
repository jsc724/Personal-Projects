{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.utils import download_asset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
    "from mir_eval import separation\n",
    "from torchaudio.transforms import Fade\n",
    "from scipy.io.wavfile import write, read\n",
    "import numpy as np\n",
    "import wave\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating track\n"
     ]
    }
   ],
   "source": [
    "importpath = \"C:/Users/jakes/Downloads/Nina Simone - Sinnerman.wav\"\n",
    "exportfolder = \"C:/Users/jakes/Music/PythonStems/Nina Simone Sinnerman/\"`\n",
    "\n",
    "def separate_sources(\n",
    "        model,\n",
    "        mix,\n",
    "        segment=10.,\n",
    "        overlap=0.1,\n",
    "        device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
    "\n",
    "    Args:\n",
    "        segment (int): segment length in seconds\n",
    "        device (torch.device, str, or None): if provided, device on which to\n",
    "            execute the computation, otherwise `mix.device` is assumed.\n",
    "            When `device` is different from `mix.device`, only local computations will\n",
    "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape='linear')\n",
    "\n",
    "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final\n",
    "\n",
    "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
    "model = bundle.get_model()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "sample_rate = bundle.sample_rate\n",
    "\n",
    "SAMPLE_SONG = download_asset(importpath)\n",
    "waveform, sample_rate = torchaudio.load(SAMPLE_SONG)\n",
    "waveform.to(device)\n",
    "mixture = waveform\n",
    "\n",
    "# parameters\n",
    "segment: int = 10\n",
    "overlap = 0.1\n",
    "\n",
    "print(\"Separating track\")\n",
    "\n",
    "ref = waveform.mean(0)\n",
    "waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
    "\n",
    "sources = separate_sources(\n",
    "    model,\n",
    "    waveform[None],\n",
    "    device=device,\n",
    "    segment=segment,\n",
    "    overlap=overlap,\n",
    ")[0]\n",
    "sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "sources_list = model.sources\n",
    "sources = list(sources)\n",
    "\n",
    "audios = dict(zip(sources_list, sources))\n",
    "\n",
    "try:\n",
    "    os.chdir(exportfolder)\n",
    "except (FileNotFoundError,FileExistsError):\n",
    "    os.makedirs(exportfolder)\n",
    "\n",
    "# Drums Audio\n",
    "with open(exportfolder+'drums.wav', 'wb') as f:\n",
    "    f.write(Audio(audios['drums'], rate=sample_rate).data)\n",
    "\n",
    "# Bass Audio\n",
    "with open(exportfolder+'bass.wav', 'wb') as f:\n",
    "    f.write(Audio(audios['bass'], rate=sample_rate).data)\n",
    "\n",
    "# Vocals Audio\n",
    "with open(exportfolder+'vocals.wav', 'wb') as f:\n",
    "    f.write(Audio(audios['vocals'], rate=sample_rate).data)\n",
    "\n",
    "# Other Audio\n",
    "with open(exportfolder+'melodies.wav', 'wb') as f:\n",
    "    f.write(Audio(audios['other'], rate=sample_rate).data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "474494322f58e04feada64ebef398262cdf533ed46e9795a581426af4f5decd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
